{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXNYox5rU2omtVi5uhHLjH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MADDIPATIGOWTHAM/FML/blob/main/noise_removal(in_Text).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.WORD TOKENIZATION USING SPLIT()**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LcKoWR3hvAHs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSWPzTHquirN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a20994-156c-4a4c-9ad8-0471fe516dc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "no.of tokens= 8\n",
            "['Once', 'upon', 'a', 'time', 'their', 'lived', 'a', 'ghost!']\n"
          ]
        }
      ],
      "source": [
        "!pip install --user -U nltk\n",
        "Text=\"Once upon a time their lived a ghost!\"\n",
        "tokens=Text.split();\n",
        "print(\"no.of tokens=\",len(tokens))\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentence tokenization\n",
        "Text1=\"ramu is a good boy.He won lot of prizes.\"\n",
        "tokens1=Text1.split(\".\")\n",
        "print(tokens1)\n",
        "print(\"No.of tokens=\",len(tokens1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hs2OeE8Cv2l1",
        "outputId": "779b764b-5273-4283-ff9e-e714f70f96f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ramu is a good boy', 'He won lot of prizes', '']\n",
            "No.of tokens= 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.TOKENIZATION FOR REGULAR EXPRESSION**"
      ],
      "metadata": {
        "id": "gY-dwhQ-yqBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#word tokenization\n",
        "import re\n",
        "Text=\"Once upon a time their lived a ghost!\"\n",
        "tokens=re.findall(\"[\\w']+\",Text)\n",
        "print(tokens)\n",
        "print(\"No.of tokens=\",len(tokens))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH6DB-ILwzNr",
        "outputId": "4adce947-c27d-4c69-e9e1-84872ca77a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Once', 'upon', 'a', 'time', 'their', 'lived', 'a', 'ghost']\n",
            "No.of tokens= 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Text=\"ramu is a good boy.He won lot of prizes.\"\n",
        "tokens=re.compile('[.?!]').split(Text)\n",
        "print(tokens)\n",
        "print(\"No.of tokens=\",len(tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8gJxJ3Qz7iz",
        "outputId": "399c1b71-3182-47df-e00f-b53a227410d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ramu is a good boy', 'He won lot of prizes', '']\n",
            "No.of tokens= 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --user -U nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8sUyMbGz7lT",
        "outputId": "a7e41681-50fa-4cec-c03e-683054667548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVHxrOc1z7rl",
        "outputId": "12e46f93-e9bb-4f14-b6e9-45035b2bf8fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tokens5=word_tokenize(Text)\n",
        "print(tokens5)\n",
        "print(\"No.of tokens=\",len(tokens5))\n"
      ],
      "metadata": {
        "id": "79xB8q7z7XGT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "784557d9-55a7-4552-ad23-b31effffbb8f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'a', 'Man', '.', 'She', 'was', 'a', 'women', '.']\n",
            "No.of tokens= 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "Text=\"I am a Man. She was a women.\"\n",
        "print(sent_tokenize(Text))\n"
      ],
      "metadata": {
        "id": "VLm1zTBj7XJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01b942c1-3305-42a0-8e14-c4a1f388a04e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I am a Man.', 'She was a women.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"God is Great. I won a lottery.\"\n",
        "print(sent_tokenize(text))\n"
      ],
      "metadata": {
        "id": "hElbYlit7XLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a23b5a0f-e82d-4e09-e667-8142d9bc28b4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['God is Great.', 'I won a lottery.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming Words\n",
        "from nltk.stem import PorterStemmer\n",
        "Porter=PorterStemmer()\n",
        "print(Porter.stem('cats'))\n",
        "print(Porter.stem('swimming'))\n"
      ],
      "metadata": {
        "id": "ChNAsBWs7XOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "778f6766-0984-41c2-d2fd-1020a67b6a0a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "swim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "Porter=PorterStemmer()\n",
        "sentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "def stem_sentence(sentence):\n",
        "  token_words=word_tokenize(sentence)\n",
        "  print(token_words)\n",
        "  sent=[]\n",
        "  for word in token_words:\n",
        "    sent.append(Porter.stem(word))\n",
        "    sent.append(\" \")\n",
        "  return \"\".join(sent)\n",
        "\n",
        "x=stem_sentence(sentence)\n",
        "print(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYsMS27VBNa8",
        "outputId": "e9159f04-f803-472e-a3a4-03ec841f873c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Pythoners', 'are', 'very', 'intelligent', 'and', 'work', 'very', 'pythonly', 'and', 'now', 'they', 'are', 'pythoning', 'their', 'way', 'to', 'success', '.']\n",
            "python are veri intellig and work veri pythonli and now they are python their way to success . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using lemmatizer for stemming\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer=WordNetLemmatizer()\n",
        "from nltk.tokenize import word_tokenize\n",
        "sentence=\"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
        "tokens=word_tokenize(sentence)\n",
        "print(tokens)\n",
        "sent=[]\n",
        "for word in tokens:\n",
        "  sent.append(wordnet_lemmatizer.lemmatize(word))\n",
        "  sent.append(\" \")\n",
        "print(\"\".join(sent))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23VjBDHXCU5H",
        "outputId": "b6e51ca1-2e83-4f0e-9715-7899883bda4c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['He', 'was', 'running', 'and', 'eating', 'at', 'same', 'time', '.', 'He', 'has', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'long', 'hours', 'in', 'the', 'Sun', '.']\n",
            "He wa running and eating at same time . He ha bad habit of swimming after playing long hour in the Sun . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ti8o0tE7G2Sa",
        "outputId": "44bc7779-ebbc-4611-c8b1-e93a91d99bf9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords=stopwords.words('english')\n",
        "import string\n",
        "sentence=\"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
        "sentence=sentence.lower()\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "Porter=PorterStemmer()\n",
        "tokens=word_tokenize(sentence)\n",
        "print(tokens)\n",
        "f_sent=[]\n",
        "for words in tokens:\n",
        "  if words not in stopwords and words not in string.punctuation:\n",
        "    f_sent.append(words)\n",
        "    f_sent.append(\" \")\n",
        "print(\"\".join(f_sent))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vzSr6yGG2VL",
        "outputId": "1fd8267d-772f-40ff-c491-c87b9312a08f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['he', 'was', 'running', 'and', 'eating', 'at', 'same', 'time', '.', 'he', 'has', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'long', 'hours', 'in', 'the', 'sun', '.']\n",
            "running eating time bad habit swimming playing long hours sun \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yLbweDwZG2aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZetBSVEeG2ce"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}